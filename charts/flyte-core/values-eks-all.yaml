userSettings:
  accountNumber: "673279459419"
  accountRegion: ap-southeast-1
  certificateArn: arn:aws:acm:ap-southeast-1:673279459419:certificate/4421f53d-e024-4959-a963-a7c46d356539
  dbPassword: flyteadmin
  rdsHost: flyteadmin.cfqkros6ju0m.ap-southeast-1.rds.amazonaws.com
  bucketName: ge-devops-bucket
  logGroup: flyteplatform

#
# FLYTEADMIN
#

flyteadmin:
  replicaCount: 1
  # -- IAM role for SA: https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html
  serviceAccount:
    # -- If the service account is created by you, make this false, else a new service account will be created and the iam-role-flyte will be added
    # you can change the name of this role
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::{{ .Values.userSettings.accountNumber }}:role/iam-role-flyte

  resources:
    limits:
      ephemeral-storage: 200Mi
    requests:
      cpu: 50m
      ephemeral-storage: 200Mi
      memory: 200Mi
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: flyteadmin
          topologyKey: kubernetes.io/hostname

#
# FLYTESCHEDULER
#

flytescheduler: {}

#
# DATACATALOG
#

datacatalog:
  replicaCount: 1
  serviceAccount:
    # -- If the service account is created by you, make this false
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::{{ .Values.userSettings.accountNumber }}:role/iam-role-flyte
  resources:
    limits:
      cpu: 1
      ephemeral-storage: 200Mi
    requests:
      cpu: 500m
      ephemeral-storage: 200Mi
      memory: 200Mi
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: datacatalog
          topologyKey: kubernetes.io/hostname

#
# FLYTEPROPELLER
#

flytepropeller:
  replicaCount: 1
  manager: false
  serviceAccount:
    # -- If the service account is created by you, make this false
    create: true
    annotations:
      eks.amazonaws.com/role-arn: arn:aws:iam::{{ .Values.userSettings.accountNumber }}:role/iam-role-flyte
  resources:
    limits:
      cpu: 1
      ephemeral-storage: 1Gi
      memory: 2Gi
    requests:
      cpu: 1
      ephemeral-storage: 1Gi
      memory: 2Gi
  cacheSizeMbs: 1024
  # -- Sets priorityClassName for propeller pod(s).
  priorityClassName: "system-cluster-critical"
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: flytepropeller
          topologyKey: kubernetes.io/hostname

#
# FLYTECONSOLE
#

flyteconsole:
  replicaCount: 1
  resources:
    limits:
      cpu: 250m
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              app.kubernetes.io/name: flyteconsole
          topologyKey: kubernetes.io/hostname

#
# COMMON
#

common:
  ingress:
    albSSLRedirect: true
    separateGrpcIngress: true
#    annotations:
#      # -- aws-load-balancer-controller v2.1 or higher is required - https://kubernetes-sigs.github.io/aws-load-balancer-controller/v2.1/
#      # For EKS if using [ALB](https://kubernetes-sigs.github.io/aws-load-balancer-controller/guide/ingress/annotations/), these annotations are set
#      kubernetes.io/ingress.class: "nginx"
#      alb.ingress.kubernetes.io/tags: service_instance=production
#      alb.ingress.kubernetes.io/scheme: internet-facing
#      # -- This is the certificate arn of the cert imported in AWS certificate manager.
#      alb.ingress.kubernetes.io/certificate-arn: "{{ .Values.userSettings.certificateArn }}"
#      alb.ingress.kubernetes.io/listen-ports: '[{"HTTP": 80}, {"HTTPS":443}]'
#      alb.ingress.kubernetes.io/actions.ssl-redirect: '{"Type": "redirect", "RedirectConfig": { "Protocol": "HTTPS", "Port": "443", "StatusCode": "HTTP_301"}}'
#      # -- Instruct ALB Controller to not create multiple load balancers (and hence maintain a single endpoint for both GRPC and Http)
#      alb.ingress.kubernetes.io/group.name: flyte
#      alb.ingress.kubernetes.io/target-type: 'ip'
    separateGrpcIngressAnnotations:
      alb.ingress.kubernetes.io/backend-protocol-version: GRPC
    tls:
      enabled: false
  databaseSecret:
    name: db-pass
    secretManifest:
      # -- Leave it empty if your secret already exists
      # Else you can create your own secret object. You can use Kubernetes secrets, else you can configure external secrets
      # For external secrets please install Necessary dependencies, like, of your choice
      # - https://github.com/hashicorp/vault
      # - https://github.com/godaddy/kubernetes-external-secrets
      apiVersion: v1
      kind: Secret
      metadata:
        name: db-pass
      type: Opaque
      stringData:
        # -- If using plain text you can provide the password here
        pass.txt: "{{ .Values.userSettings.dbPassword }}"

# -----------------------------------------------------
# Core dependencies that should be configured for Flyte to work on any platform
# Specifically 2 - Storage (s3, gcs etc), Production RDBMS - Aurora, CloudSQL etc
# ------------------------------------------------------
#
# STORAGE SETTINGS
#

storage:
  # -- Sets the storage type. Supported values are sandbox, s3, gcs and custom.
  type: s3
  # -- bucketName defines the storage bucket flyte will use. Required for all types except for sandbox.
  bucketName: "{{ .Values.userSettings.bucketName }}"
  s3:
    region: "{{ .Values.userSettings.accountRegion }}"

db:
  datacatalog:
    database:
      port: 5432
      # -- Create a user called flyteadmin
      username: flyteadmin
      host: "{{ .Values.userSettings.rdsHost }}"
      # -- Create a DB called datacatalog (OR change the name here)
      dbname: flyteadmin
      passwordPath: /etc/db/pass.txt
  admin:
    database:
      port: 5432
      # -- Create a user called flyteadmin
      username: flyteadmin
      host: "{{ .Values.userSettings.rdsHost }}"
      # -- Create a DB called flyteadmin (OR change the name here)
      dbname: flyteadmin
      passwordPath: /etc/db/pass.txt
#
# CONFIGMAPS
#

configmap:
  adminServer:
    server:
      httpPort: 8088
      grpcPort: 8089
      security:
        secure: false
        useAuth: false
        allowCors: true
        allowedOrigins:
          # Accepting all domains for Sandbox installation
          - "*"
        allowedHeaders:
          - "Content-Type"

  task_resource_defaults:
    task_resources:
      defaults:
        cpu: 1000m
        memory: 1000Mi
        storage: 1000Mi
      limits:
        storage: 2000Mi

  core:
    propeller:
      rawoutput-prefix: "s3://{{ .Values.userSettings.bucketName }}/"
      workers: 40
      gc-interval: 12h
      max-workflow-retries: 50
      max-ttl-hours: 23
      kube-client-config:
        qps: 100
        burst: 25
        timeout: 30s
      queue:
        sub-queue:
          type: bucket
          rate: 100
          capacity: 1000

  # -- Resource manager configuration
  resource_manager:
    # -- resource manager configuration
    propeller:
      resourcemanager:
        type: noop
        # Note: By default resource manager is disable for propeller, Please use `type: redis` to enaable
        # type: redis
        # resourceMaxQuota: 10000
        # redis:
        #   hostPath: "{{ .Values.userSettings.redisHostUrl }}"
        #   hostKey: "{{ .Values.userSettings.redisHostKey }}"


  enabled_plugins:
    # -- Tasks specific configuration [structure](https://pkg.go.dev/github.com/flyteorg/flytepropeller/pkg/controller/nodes/task/config#GetConfig)
    tasks:
      # -- Plugins configuration, [structure](https://pkg.go.dev/github.com/flyteorg/flytepropeller/pkg/controller/nodes/task/config#TaskPluginConfig)
      task-plugins:
        # -- [Enabled Plugins](https://pkg.go.dev/github.com/lyft/flyteplugins/go/tasks/config#Config). Enable sagemaker*, athena if you install the backend
        # plugins
        enabled-plugins:
          - container
          - sidecar
          - k8s-array
          - spark
        #          - sagemaker_hyperparameter_tuning
        #          - sagemaker_custom_training
        #          - sagemaker_training
        default-for-task-types:
          container: container
          sidecar: sidecar
          container_array: k8s-array
          spark: spark
  #          sagemaker_custom_training_task: sagemaker_custom_training
  #          sagemaker_custom_training_job_task: sagemaker_custom_training


  # -- Section that configures how the Task logs are displayed on the UI. This has to be changed based on your actual logging provider.
  # Refer to [structure](https://pkg.go.dev/github.com/lyft/flyteplugins/go/tasks/logs#LogConfig) to understand how to configure various
  # logging engines
  task_logs:
    plugins:
      logs:
        kubernetes-enabled: false
        # -- One option is to enable cloudwatch logging for EKS, update the region and log group accordingly
        # You can even disable this
        cloudwatch-enabled: true
        # -- region where logs are hosted
        cloudwatch-region: "{{ .Values.userSettings.accountRegion }}"
        # -- cloudwatch log-group
        cloudwatch-log-group: "{{ .Values.userSettings.logGroup }}"

# ----------------------------------------------------------------
# Optional Modules
# Flyte built extensions that enable various additional features in Flyte.
# All these features are optional, but are critical to run certain features
# ------------------------------------------------------------------------

# -- # Flyte uses a cloud hosted Cron scheduler to run workflows on a schedule. The following module is optional. Without,
# this module, you will not have scheduled launchplans/workflows.
# by default aws use flyte native scheduler i.e flytescheduler
# To use aws native scheduler please override the eks values
workflow_scheduler:
  enabled: true
  type: native

# --
# Workflow notifications module is an optional dependency. Flyte uses cloud native pub-sub systems to notify users of
# various events in their workflows
workflow_notifications:
  enabled: true
  config:
    notifications:
      type: aws
      region: "ap-southeast-1"
      publisher:
        topicName: "arn:aws:sns:ap-southeast-1:673279459419:flyte-notifications-topic"
      processor:
        queueName: flyte-notifications-queue
        accountId: "673279459419"
      emailer:
        subject: "Flyte: {{ project }}/{{ domain }}/{{ launch_plan.name }} has '{{ phase }}'"
        sender: "nguyenhuuvan.39@gmail.com"
        body: |
          Execution {{ workflow.project }}/{{ workflow.domain }}/{{ workflow.name }}/{{ name }} has {{ phase }}.
          Details: https://flyte.example.com/console/projects/{{ project }}/domains/{{ domain }}/executions/{{ name }}.
          {{ error }}

# -- Configuration for the Cluster resource manager component. This is an optional component, that enables automatic
# cluster configuration. This is useful to set default quotas, manage namespaces etc that map to a project/domain
cluster_resource_manager:
  # -- Enables the Cluster resource manager component
  enabled: true
  # -- Starts the cluster resource manager in standalone mode with requisite auth credentials to call flyteadmin service endpoints
  standalone_deploy: false
  config:
    # -- ClusterResource parameters
    # Refer to the [structure](https://pkg.go.dev/github.com/lyft/flyteadmin@v0.3.37/pkg/runtime/interfaces#ClusterResourceConfig) to customize.
    cluster_resources:
      refreshInterval: 5m
      templatePath: "/etc/flyte/clusterresource/templates"
      customData:
        - production:
            - projectQuotaCpu:
                value: "1"
            - projectQuotaMemory:
                value: "1000Mi"
        - staging:
            - projectQuotaCpu:
                value: "1"
            - projectQuotaMemory:
                value: "1000Mi"
        - development:
            - projectQuotaCpu:
                value: "1"
            - projectQuotaMemory:
                value: "1000Mi"
      refresh: 5m

  # -- Resource templates that should be applied
  templates:
    # -- Template for namespaces resources
    - key: aa_namespace
      value: |
        apiVersion: v1
        kind: Namespace
        metadata:
          name: {{ namespace }}
        spec:
          finalizers:
          - kubernetes

    - key: ab_project_resource_quota
      value: |
        apiVersion: v1
        kind: ResourceQuota
        metadata:
          name: project-quota
          namespace: {{ namespace }}
        spec:
          hard:
            limits.cpu: {{ projectQuotaCpu }}
            limits.memory: {{ projectQuotaMemory }}

    - key: ac_spark_role
      value: |
        apiVersion: rbac.authorization.k8s.io/v1
        kind: Role
        metadata:
          name: spark-role
          namespace: {{ namespace }}
        rules:
        - apiGroups: ["*"]
          resources:
          - pods
          verbs:
          - '*'
        - apiGroups: ["*"]
          resources:
          - services
          verbs:
          - '*'
        - apiGroups: ["*"]
          resources:
          - configmaps
          verbs:
          - '*'

    - key: ad_spark_service_account
      value: |
        apiVersion: v1
        kind: ServiceAccount
        metadata:
          name: spark
          namespace: {{ namespace }}

    - key: ae_spark_role_binding
      value: |
        apiVersion: rbac.authorization.k8s.io/v1
        kind: RoleBinding
        metadata:
          name: spark-role-binding
          namespace: {{ namespace }}
        roleRef:
          apiGroup: rbac.authorization.k8s.io
          kind: Role
          name: spark-role
        subjects:
        - kind: ServiceAccount
          name: spark
          namespace: {{ namespace }}

sparkoperator:
  enabled: true
  plugin_config:
    plugins:
      spark:
        # -- Spark default configuration
        spark-config-default:
          # We override the default credentials chain provider for Hadoop so that
          # it can use the serviceAccount based IAM role or ec2 metadata based.
          # This is more in line with how AWS works
          - spark.hadoop.fs.s3a.aws.credentials.provider: "com.amazonaws.auth.DefaultAWSCredentialsProviderChain"
          - spark.hadoop.mapreduce.fileoutputcommitter.algorithm.version: "2"
          - spark.kubernetes.allocation.batch.size: "50"
          - spark.hadoop.fs.s3a.acl.default: "BucketOwnerFullControl"
          - spark.hadoop.fs.s3n.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
          - spark.hadoop.fs.AbstractFileSystem.s3n.impl: "org.apache.hadoop.fs.s3a.S3A"
          - spark.hadoop.fs.s3.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
          - spark.hadoop.fs.AbstractFileSystem.s3.impl: "org.apache.hadoop.fs.s3a.S3A"
          - spark.hadoop.fs.s3a.impl: "org.apache.hadoop.fs.s3a.S3AFileSystem"
          - spark.hadoop.fs.AbstractFileSystem.s3a.impl: "org.apache.hadoop.fs.s3a.S3A"
          - spark.hadoop.fs.s3a.multipart.threshold: "536870912"
          - spark.excludeOnFailure.enabled: "true"
          - spark.excludeOnFailure.timeout: "5m"
          - spark.task.maxfailures: "8"